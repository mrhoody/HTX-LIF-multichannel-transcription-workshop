{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Whisper Multichannel Transcription Workshop\n",
        "\n",
        "Welcome to this comprehensive tutorial on audio transcription using OpenAI's Whisper model!\n",
        "\n",
        "**Workshop Outline:**\n",
        "1. Speech-to-Text Primer\n",
        "2. Whisper Introduction\n",
        "3. Single-Speaker Demo\n",
        "4. Multi-Speaker Demo\n",
        "5. Speaker Diarization\n",
        "6. Multichannel Audio Processing\n",
        "\n",
        "**Prerequisites:**\n",
        "- Basic Python knowledge\n",
        "- Understanding of audio concepts is helpful but not required\n",
        "- All necessary packages installed (see requirements.txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Library Imports\n",
        "\n",
        "First, let's import all the necessary libraries we'll be using throughout this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display, Markdown\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# Whisper for transcription\n",
        "import whisper\n",
        "\n",
        "# System utilities\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\u2713 All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Speech-to-Text Primer\n",
        "\n",
        "### What is Speech-to-Text?\n",
        "\n",
        "Speech-to-Text (STT), also known as Automatic Speech Recognition (ASR), is the technology that converts spoken language into written text.\n",
        "\n",
        "### How Does It Work?\n",
        "\n",
        "Modern ASR systems typically involve several stages:\n",
        "\n",
        "1. **Audio Preprocessing**: Converting raw audio into a suitable format\n",
        "2. **Feature Extraction**: Extracting relevant acoustic features (e.g., spectrograms, MFCCs)\n",
        "3. **Acoustic Modeling**: Using deep learning models to map features to phonemes or characters\n",
        "4. **Language Modeling**: Applying linguistic knowledge to improve accuracy\n",
        "5. **Decoding**: Converting predictions into final text output\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Sample Rate**: Number of audio samples per second (e.g., 16kHz)\n",
        "- **Spectrogram**: Visual representation of audio frequencies over time\n",
        "- **Mel-Frequency Cepstral Coefficients (MFCCs)**: Compact audio feature representation\n",
        "- **Word Error Rate (WER)**: Common metric for ASR accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Audio\n",
        "\n",
        "Let's create a simple example to understand audio data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a simple sine wave as an example audio signal\n",
        "sample_rate = 16000  # 16kHz sample rate\n",
        "duration = 2  # seconds\n",
        "frequency = 440  # A4 note (440 Hz)\n",
        "\n",
        "# Create time array\n",
        "t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "\n",
        "# Generate sine wave\n",
        "audio_signal = np.sin(2 * np.pi * frequency * t)\n",
        "\n",
        "# Plot waveform\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(t[:1000], audio_signal[:1000])  # Plot first 1000 samples\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Audio Waveform (First 1000 samples)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Create and plot spectrogram\n",
        "plt.subplot(1, 2, 2)\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_signal)), ref=np.max)\n",
        "librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='hz')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Spectrogram')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Play the audio\n",
        "display(Audio(audio_signal, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Explore Audio Properties\n",
        "\n",
        "Try modifying the frequency or duration in the code above to see how it affects the waveform and spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Experiment with different frequencies and durations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Introduction to Whisper\n",
        "\n",
        "### What is Whisper?\n",
        "\n",
        "Whisper is a state-of-the-art automatic speech recognition (ASR) system developed by OpenAI. It's trained on 680,000 hours of multilingual and multitask supervised data collected from the web.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Multilingual**: Supports 99+ languages\n",
        "- **Robust**: Works well with various accents and background noise\n",
        "- **Zero-shot**: No fine-tuning required for many tasks\n",
        "- **Multi-task**: Can transcribe, translate, and identify languages\n",
        "\n",
        "### Model Sizes\n",
        "\n",
        "Whisper comes in several sizes, trading off speed vs. accuracy:\n",
        "\n",
        "| Model  | Parameters | English-only | Multilingual | Required VRAM | Relative Speed |\n",
        "|--------|------------|--------------|--------------|---------------|----------------|\n",
        "| tiny   | 39 M       | \u2713            | \u2713            | ~1 GB         | ~32x           |\n",
        "| base   | 74 M       | \u2713            | \u2713            | ~1 GB         | ~16x           |\n",
        "| small  | 244 M      | \u2713            | \u2713            | ~2 GB         | ~6x            |\n",
        "| medium | 769 M      | \u2713            | \u2713            | ~5 GB         | ~2x            |\n",
        "| large  | 1550 M     | \u2717            | \u2713            | ~10 GB        | 1x             |\n",
        "\n",
        "### Architecture\n",
        "\n",
        "Whisper uses a Transformer encoder-decoder architecture:\n",
        "- **Encoder**: Processes audio features (mel spectrograms)\n",
        "- **Decoder**: Generates text transcriptions autoregressively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading a Whisper Model\n",
        "\n",
        "Let's load a Whisper model. We'll start with the 'base' model for a good balance of speed and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Whisper model\n",
        "# Options: 'tiny', 'base', 'small', 'medium', 'large'\n",
        "model_size = 'base'\n",
        "\n",
        "print(f\"Loading Whisper '{model_size}' model...\")\n",
        "model = whisper.load_model(model_size)\n",
        "print(f\"\u2713 Model loaded successfully!\")\n",
        "print(f\"  Device: {model.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Whisper Options\n",
        "\n",
        "Whisper provides several options for transcription:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key transcription options\n",
        "transcription_options = {\n",
        "    'language': 'en',  # Specify language (or None for auto-detect)\n",
        "    'task': 'transcribe',  # 'transcribe' or 'translate' (to English)\n",
        "    'temperature': 0.0,  # Lower = more deterministic\n",
        "    'beam_size': 5,  # Number of beams for beam search\n",
        "    'best_of': 5,  # Number of candidates to generate\n",
        "    'fp16': True,  # Use half-precision (faster, requires GPU)\n",
        "    'verbose': False,  # Show progress\n",
        "}\n",
        "\n",
        "print(\"Transcription Options:\")\n",
        "for key, value in transcription_options.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Model Selector\n",
        "\n",
        "Use this widget to explore different model sizes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive model selector\n",
        "model_selector = widgets.Dropdown(\n",
        "    options=['tiny', 'base', 'small', 'medium', 'large'],\n",
        "    value='base',\n",
        "    description='Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "def on_model_change(change):\n",
        "    print(f\"Selected model: {change['new']}\")\n",
        "    print(f\"Note: Larger models provide better accuracy but require more memory and time.\")\n",
        "\n",
        "model_selector.observe(on_model_change, names='value')\n",
        "display(model_selector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Single-Speaker Transcription Demo\n",
        "\n",
        "Now let's put Whisper to work! We'll start with a simple single-speaker audio example.\n",
        "\n",
        "### Creating Sample Audio\n",
        "\n",
        "For demonstration purposes, we'll create a simple audio file using text-to-speech, or you can use your own audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to create a sample audio file\n",
        "def create_sample_audio():\n",
        "    \"\"\"\n",
        "    This is a placeholder for creating sample audio.\n",
        "    In practice, you would:\n",
        "    1. Record your own audio\n",
        "    2. Download sample files\n",
        "    3. Use existing audio files\n",
        "    \"\"\"\n",
        "    print(\"To use this demo:\")\n",
        "    print(\"1. Record a short audio clip (e.g., using your phone or computer)\")\n",
        "    print(\"2. Save it as a .wav or .mp3 file\")\n",
        "    print(\"3. Place it in the notebooks/ directory\")\n",
        "    print(\"4. Update the 'audio_path' variable below\")\n",
        "    \n",
        "create_sample_audio()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading and Analyzing Audio\n",
        "\n",
        "Let's load an audio file and examine its properties:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify your audio file path\n",
        "# audio_path = 'sample_single_speaker.wav'\n",
        "\n",
        "# For this demo, we'll create a simple example\n",
        "# Replace this with your actual audio file path\n",
        "audio_path = None  # Set to your audio file path\n",
        "\n",
        "if audio_path and os.path.exists(audio_path):\n",
        "    # Load audio\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    \n",
        "    # Display audio properties\n",
        "    duration = len(audio) / sr\n",
        "    print(f\"Audio Properties:\")\n",
        "    print(f\"  Sample Rate: {sr} Hz\")\n",
        "    print(f\"  Duration: {duration:.2f} seconds\")\n",
        "    print(f\"  Samples: {len(audio)}\")\n",
        "    print(f\"  Channels: 1 (mono)\")\n",
        "    \n",
        "    # Visualize waveform\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.waveshow(audio, sr=sr)\n",
        "    plt.title('Audio Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Play audio\n",
        "    display(Audio(audio, rate=sr))\n",
        "else:\n",
        "    print(\"No audio file specified or file not found.\")\n",
        "    print(\"Please set 'audio_path' to your audio file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transcribing Single-Speaker Audio\n",
        "\n",
        "Now let's transcribe the audio using Whisper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if audio_path and os.path.exists(audio_path):\n",
        "    # Transcribe audio\n",
        "    print(\"Transcribing audio...\")\n",
        "    result = model.transcribe(audio_path, language='en', fp16=False)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRANSCRIPTION RESULT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nDetected Language: {result.get('language', 'N/A')}\")\n",
        "    print(f\"\\nText:\\n{result['text']}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "else:\n",
        "    print(\"Please provide an audio file to transcribe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Transcription with Timestamps\n",
        "\n",
        "Whisper can also provide word-level or segment-level timestamps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if audio_path and os.path.exists(audio_path):\n",
        "    # Display segments with timestamps\n",
        "    print(\"Segments with Timestamps:\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, segment in enumerate(result['segments'], 1):\n",
        "        start_time = segment['start']\n",
        "        end_time = segment['end']\n",
        "        text = segment['text'].strip()\n",
        "        \n",
        "        print(f\"[{start_time:6.2f}s - {end_time:6.2f}s] {text}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"Please provide an audio file to transcribe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Try Your Own Audio\n",
        "\n",
        "Record a short clip of yourself speaking and transcribe it using the code above!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Load and transcribe your own audio file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Multi-Speaker Transcription Demo\n",
        "\n",
        "Multi-speaker audio presents additional challenges:\n",
        "- Overlapping speech\n",
        "- Different voice characteristics\n",
        "- Need to identify \"who said what\"\n",
        "\n",
        "While Whisper excels at transcription, it doesn't inherently identify different speakers. For that, we need **speaker diarization** (covered in the next section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Multi-Speaker Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify your multi-speaker audio file path\n",
        "# multi_speaker_path = 'sample_multi_speaker.wav'\n",
        "\n",
        "multi_speaker_path = None  # Set to your audio file path\n",
        "\n",
        "if multi_speaker_path and os.path.exists(multi_speaker_path):\n",
        "    # Load audio\n",
        "    audio_multi, sr_multi = librosa.load(multi_speaker_path, sr=16000)\n",
        "    \n",
        "    # Display audio properties\n",
        "    duration_multi = len(audio_multi) / sr_multi\n",
        "    print(f\"Multi-Speaker Audio Properties:\")\n",
        "    print(f\"  Duration: {duration_multi:.2f} seconds\")\n",
        "    print(f\"  Sample Rate: {sr_multi} Hz\")\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.waveshow(audio_multi, sr=sr_multi)\n",
        "    plt.title('Multi-Speaker Audio Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Play audio\n",
        "    display(Audio(audio_multi, rate=sr_multi))\n",
        "else:\n",
        "    print(\"No multi-speaker audio file specified.\")\n",
        "    print(\"Please set 'multi_speaker_path' to your audio file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transcribing Multi-Speaker Audio\n",
        "\n",
        "Whisper will transcribe all speech, but won't separate speakers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if multi_speaker_path and os.path.exists(multi_speaker_path):\n",
        "    # Transcribe\n",
        "    print(\"Transcribing multi-speaker audio...\")\n",
        "    result_multi = model.transcribe(multi_speaker_path, language='en', fp16=False)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MULTI-SPEAKER TRANSCRIPTION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n{result_multi['text']}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\nNote: This transcription includes all speakers,\")\n",
        "    print(\"but doesn't identify who is speaking.\")\n",
        "    print(\"See the next section on diarization for speaker identification.\")\n",
        "else:\n",
        "    print(\"Please provide a multi-speaker audio file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenges with Multi-Speaker Audio\n",
        "\n",
        "Consider these challenges:\n",
        "- **Speaker overlap**: When multiple people talk simultaneously\n",
        "- **Turn-taking**: Rapid exchanges between speakers\n",
        "- **Background noise**: Multiple voices in the background\n",
        "- **Speaker identification**: Determining who said what"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise space: Analyze transcription segments\n",
        "# Can you identify potential speaker changes by analyzing\n",
        "# pauses or changes in the audio signal?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Speaker Diarization\n",
        "\n",
        "### What is Speaker Diarization?\n",
        "\n",
        "Speaker diarization is the process of partitioning an audio stream into homogeneous segments according to the speaker identity. In simpler terms: **\"Who spoke when?\"**\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **Speech Activity Detection (SAD)**: Identifying speech vs. non-speech regions\n",
        "2. **Speaker Segmentation**: Detecting speaker change points\n",
        "3. **Speaker Clustering**: Grouping segments by speaker\n",
        "4. **Speaker Labeling**: Assigning labels (Speaker 1, Speaker 2, etc.)\n",
        "\n",
        "### Tools for Diarization\n",
        "\n",
        "- **pyannote.audio**: State-of-the-art speaker diarization toolkit\n",
        "- **Resemblyzer**: Speaker embedding and verification\n",
        "- **speechbrain**: End-to-end speech processing toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using pyannote.audio for Diarization\n",
        "\n",
        "Let's use pyannote.audio to perform speaker diarization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import pyannote.audio\n",
        "try:\n",
        "    from pyannote.audio import Pipeline\n",
        "    pyannote_available = True\n",
        "except ImportError:\n",
        "    print(\"pyannote.audio not available. Install with: pip install pyannote.audio\")\n",
        "    pyannote_available = False\n",
        "\n",
        "# Note: You'll need a Hugging Face token for pyannote models\n",
        "# Get it from: https://huggingface.co/settings/tokens\n",
        "# Accept terms at: https://huggingface.co/pyannote/speaker-diarization\n",
        "\n",
        "HF_TOKEN = os.environ.get('HF_TOKEN', None)\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"\u26a0\ufe0f  Warning: HF_TOKEN not set.\")\n",
        "    print(\"To use pyannote.audio:\")\n",
        "    print(\"1. Create a Hugging Face account\")\n",
        "    print(\"2. Accept conditions at https://huggingface.co/pyannote/speaker-diarization\")\n",
        "    print(\"3. Create a token at https://huggingface.co/settings/tokens\")\n",
        "    print(\"4. Set it: export HF_TOKEN=your_token_here\")\n",
        "else:\n",
        "    print(\"\u2713 HF_TOKEN found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diarization pipeline\n",
        "if pyannote_available and HF_TOKEN:\n",
        "    try:\n",
        "        print(\"Loading speaker diarization pipeline...\")\n",
        "        diarization_pipeline = Pipeline.from_pretrained(\n",
        "            \"pyannote/speaker-diarization-3.1\",\n",
        "            use_auth_token=HF_TOKEN\n",
        "        )\n",
        "        print(\"\u2713 Pipeline loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pipeline: {e}\")\n",
        "        diarization_pipeline = None\n",
        "else:\n",
        "    diarization_pipeline = None\n",
        "    print(\"Diarization pipeline not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performing Diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if diarization_pipeline and multi_speaker_path and os.path.exists(multi_speaker_path):\n",
        "    # Perform diarization\n",
        "    print(\"Performing speaker diarization...\")\n",
        "    diarization = diarization_pipeline(multi_speaker_path)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SPEAKER DIARIZATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        print(f\"[{turn.start:6.2f}s - {turn.end:6.2f}s] {speaker}\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"Diarization not available or no audio file provided.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining Diarization with Transcription\n",
        "\n",
        "Now let's combine speaker diarization with Whisper transcription to get \"who said what\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combine_diarization_transcription(audio_path, diarization, transcription):\n",
        "    \"\"\"\n",
        "    Combine diarization results with transcription to attribute text to speakers.\n",
        "    \n",
        "    Args:\n",
        "        audio_path: Path to audio file\n",
        "        diarization: Diarization results from pyannote\n",
        "        transcription: Transcription results from Whisper\n",
        "    \n",
        "    Returns:\n",
        "        List of dictionaries with speaker, start, end, and text\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Get segments from transcription\n",
        "    for segment in transcription['segments']:\n",
        "        seg_start = segment['start']\n",
        "        seg_end = segment['end']\n",
        "        seg_text = segment['text'].strip()\n",
        "        \n",
        "        # Find overlapping speaker from diarization\n",
        "        speaker = \"Unknown\"\n",
        "        max_overlap = 0\n",
        "        \n",
        "        for turn, _, spk in diarization.itertracks(yield_label=True):\n",
        "            # Calculate overlap\n",
        "            overlap_start = max(seg_start, turn.start)\n",
        "            overlap_end = min(seg_end, turn.end)\n",
        "            overlap = max(0, overlap_end - overlap_start)\n",
        "            \n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                speaker = spk\n",
        "        \n",
        "        results.append({\n",
        "            'speaker': speaker,\n",
        "            'start': seg_start,\n",
        "            'end': seg_end,\n",
        "            'text': seg_text\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "if diarization_pipeline and multi_speaker_path and os.path.exists(multi_speaker_path):\n",
        "    combined_results = combine_diarization_transcription(\n",
        "        multi_speaker_path, diarization, result_multi\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMBINED DIARIZATION + TRANSCRIPTION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for result in combined_results:\n",
        "        print(f\"\\n[{result['start']:6.2f}s - {result['end']:6.2f}s] {result['speaker']}\")\n",
        "        print(f\"  {result['text']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "else:\n",
        "    print(\"Combined processing not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Improve Speaker Attribution\n",
        "\n",
        "The simple overlap method above might not be perfect. Try to improve it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Ideas:\n",
        "# - Use weighted overlap based on segment length\n",
        "# - Consider speaker changes within long segments\n",
        "# - Add confidence scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Multichannel Audio Processing\n",
        "\n",
        "### What is Multichannel Audio?\n",
        "\n",
        "Multichannel audio contains multiple independent audio channels, often recorded from different microphones or sources:\n",
        "\n",
        "- **Stereo (2 channels)**: Left and right\n",
        "- **Surround sound (5.1, 7.1)**: Multiple spatial channels\n",
        "- **Multi-mic recordings**: Each microphone on a separate channel\n",
        "\n",
        "### Why Process Channels Separately?\n",
        "\n",
        "Benefits of per-channel processing:\n",
        "1. **Speaker separation**: Different speakers on different channels\n",
        "2. **Noise reduction**: Better quality on specific channels\n",
        "3. **Spatial information**: Maintain location/direction data\n",
        "4. **Improved accuracy**: Cleaner input for ASR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Multichannel Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load multichannel audio (keeping all channels)\n",
        "# multichannel_path = 'sample_multichannel.wav'\n",
        "\n",
        "multichannel_path = None  # Set to your audio file path\n",
        "\n",
        "if multichannel_path and os.path.exists(multichannel_path):\n",
        "    # Load with soundfile to preserve channels\n",
        "    audio_data, sample_rate = sf.read(multichannel_path)\n",
        "    \n",
        "    # Check if audio is multichannel\n",
        "    if len(audio_data.shape) == 1:\n",
        "        print(\"This is a mono audio file (1 channel).\")\n",
        "        n_channels = 1\n",
        "    else:\n",
        "        n_channels = audio_data.shape[1]\n",
        "        print(f\"This is a multichannel audio file with {n_channels} channels.\")\n",
        "    \n",
        "    print(f\"\\nAudio Properties:\")\n",
        "    print(f\"  Sample Rate: {sample_rate} Hz\")\n",
        "    print(f\"  Duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
        "    print(f\"  Channels: {n_channels}\")\n",
        "    print(f\"  Shape: {audio_data.shape}\")\n",
        "else:\n",
        "    print(\"No multichannel audio file specified.\")\n",
        "    print(\"For this demo, we'll create a synthetic multichannel example.\")\n",
        "    \n",
        "    # Create synthetic stereo audio for demonstration\n",
        "    duration = 5\n",
        "    sample_rate = 16000\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
        "    \n",
        "    # Left channel: 440 Hz\n",
        "    left_channel = np.sin(2 * np.pi * 440 * t)\n",
        "    # Right channel: 880 Hz\n",
        "    right_channel = np.sin(2 * np.pi * 880 * t)\n",
        "    \n",
        "    # Combine into stereo\n",
        "    audio_data = np.column_stack((left_channel, right_channel))\n",
        "    n_channels = 2\n",
        "    \n",
        "    print(\"Created synthetic stereo audio for demonstration.\")\n",
        "    print(f\"  Channels: {n_channels}\")\n",
        "    print(f\"  Left channel: 440 Hz tone\")\n",
        "    print(f\"  Right channel: 880 Hz tone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Multiple Channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize each channel\n",
        "if len(audio_data.shape) > 1 and audio_data.shape[1] > 1:\n",
        "    fig, axes = plt.subplots(n_channels, 1, figsize=(12, 3 * n_channels))\n",
        "    \n",
        "    if n_channels == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i in range(min(n_channels, 4)):  # Limit to 4 channels for display\n",
        "        channel_data = audio_data[:, i]\n",
        "        axes[i].plot(np.arange(len(channel_data)) / sample_rate, channel_data)\n",
        "        axes[i].set_title(f'Channel {i+1}')\n",
        "        axes[i].set_xlabel('Time (s)')\n",
        "        axes[i].set_ylabel('Amplitude')\n",
        "        axes[i].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Play each channel\n",
        "    for i in range(min(n_channels, 4)):\n",
        "        print(f\"\\nChannel {i+1}:\")\n",
        "        display(Audio(audio_data[:, i], rate=sample_rate))\n",
        "else:\n",
        "    print(\"Mono audio - single channel only.\")\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.plot(np.arange(len(audio_data)) / sample_rate, audio_data)\n",
        "    plt.title('Audio Waveform (Mono)')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    display(Audio(audio_data, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting and Saving Channels\n",
        "\n",
        "Let's split the multichannel audio into separate files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for channel files\n",
        "output_dir = 'channel_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Split and save channels\n",
        "if len(audio_data.shape) > 1 and audio_data.shape[1] > 1:\n",
        "    channel_files = []\n",
        "    \n",
        "    for i in range(n_channels):\n",
        "        # Extract channel\n",
        "        channel_audio = audio_data[:, i]\n",
        "        \n",
        "        # Save to file\n",
        "        output_path = os.path.join(output_dir, f'channel_{i+1}.wav')\n",
        "        sf.write(output_path, channel_audio, sample_rate)\n",
        "        channel_files.append(output_path)\n",
        "        \n",
        "        print(f\"\u2713 Saved channel {i+1} to {output_path}\")\n",
        "    \n",
        "    print(f\"\\nAll {n_channels} channels saved to '{output_dir}/' directory.\")\n",
        "else:\n",
        "    print(\"Mono audio - no splitting needed.\")\n",
        "    channel_files = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transcribing Each Channel Separately\n",
        "\n",
        "Now let's transcribe each channel independently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transcribe each channel\n",
        "if channel_files:\n",
        "    channel_transcriptions = []\n",
        "    \n",
        "    print(\"Transcribing each channel...\\n\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, channel_path in enumerate(channel_files, 1):\n",
        "        print(f\"\\nChannel {i}:\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        # Transcribe\n",
        "        result = model.transcribe(channel_path, language='en', fp16=False)\n",
        "        channel_transcriptions.append(result)\n",
        "        \n",
        "        # Display\n",
        "        print(f\"Text: {result['text']}\")\n",
        "        \n",
        "        # Show segments if available\n",
        "        if len(result['segments']) > 1:\n",
        "            print(\"\\nSegments:\")\n",
        "            for seg in result['segments']:\n",
        "                print(f\"  [{seg['start']:6.2f}s - {seg['end']:6.2f}s] {seg['text'].strip()}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"\u2713 All channels transcribed!\")\n",
        "else:\n",
        "    print(\"No channel files available for transcription.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Channel Transcriptions\n",
        "\n",
        "Let's create a comparison view of all channel transcriptions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "if channel_transcriptions:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CHANNEL TRANSCRIPTION COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, transcription in enumerate(channel_transcriptions, 1):\n",
        "        print(f\"\\nChannel {i}:\")\n",
        "        print(f\"  {transcription['text']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "    # Calculate some statistics\n",
        "    print(\"\\nStatistics:\")\n",
        "    for i, transcription in enumerate(channel_transcriptions, 1):\n",
        "        word_count = len(transcription['text'].split())\n",
        "        duration = transcription['segments'][-1]['end'] if transcription['segments'] else 0\n",
        "        print(f\"  Channel {i}: {word_count} words, {duration:.2f}s duration\")\n",
        "else:\n",
        "    print(\"No transcriptions available for comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced: Synchronized Multi-Channel Transcription\n",
        "\n",
        "For more advanced use cases, you might want to synchronize transcriptions across channels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_synchronized_transcript(channel_transcriptions):\n",
        "    \"\"\"\n",
        "    Create a synchronized transcript showing all channels aligned by time.\n",
        "    \n",
        "    Args:\n",
        "        channel_transcriptions: List of transcription results, one per channel\n",
        "    \n",
        "    Returns:\n",
        "        List of time-aligned transcript entries\n",
        "    \"\"\"\n",
        "    # Collect all timestamps from all channels\n",
        "    all_events = []\n",
        "    \n",
        "    for ch_idx, transcription in enumerate(channel_transcriptions):\n",
        "        for segment in transcription['segments']:\n",
        "            all_events.append({\n",
        "                'channel': ch_idx + 1,\n",
        "                'start': segment['start'],\n",
        "                'end': segment['end'],\n",
        "                'text': segment['text'].strip()\n",
        "            })\n",
        "    \n",
        "    # Sort by start time\n",
        "    all_events.sort(key=lambda x: x['start'])\n",
        "    \n",
        "    return all_events\n",
        "\n",
        "# Create synchronized view\n",
        "if channel_transcriptions:\n",
        "    sync_transcript = create_synchronized_transcript(channel_transcriptions)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNCHRONIZED MULTI-CHANNEL TRANSCRIPT\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for event in sync_transcript:\n",
        "        print(f\"\\n[{event['start']:6.2f}s - {event['end']:6.2f}s] Channel {event['channel']}\")\n",
        "        print(f\"  {event['text']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "else:\n",
        "    print(\"No transcriptions available for synchronization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Your Own Multichannel Project\n",
        "\n",
        "Now it's your turn! Try processing your own multichannel audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Ideas:\n",
        "# - Record a stereo conversation (each person closer to one mic)\n",
        "# - Process a multi-track recording\n",
        "# - Combine channel separation with diarization\n",
        "# - Export results to a formatted document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Conclusion and Next Steps\n",
        "\n",
        "### What We've Learned\n",
        "\n",
        "In this workshop, we covered:\n",
        "\n",
        "1. \u2713 **Speech-to-Text Fundamentals**: Understanding ASR concepts and audio processing\n",
        "2. \u2713 **Whisper Model**: Using OpenAI's powerful transcription system\n",
        "3. \u2713 **Single-Speaker Transcription**: Basic transcription workflows\n",
        "4. \u2713 **Multi-Speaker Audio**: Challenges and approaches for multiple speakers\n",
        "5. \u2713 **Speaker Diarization**: Identifying \"who spoke when\" using pyannote.audio\n",
        "6. \u2713 **Multichannel Processing**: Splitting and transcribing individual audio channels\n",
        "\n",
        "### Next Steps and Advanced Topics\n",
        "\n",
        "Continue your learning journey:\n",
        "\n",
        "#### Improving Accuracy\n",
        "- Fine-tune Whisper on domain-specific data\n",
        "- Use larger models for better performance\n",
        "- Apply audio preprocessing (noise reduction, normalization)\n",
        "\n",
        "#### Advanced Techniques\n",
        "- Real-time streaming transcription\n",
        "- Custom vocabulary and named entity recognition\n",
        "- Multi-language support and code-switching\n",
        "- Speaker identification (not just diarization)\n",
        "\n",
        "#### Integration Projects\n",
        "- Build a meeting transcription system\n",
        "- Create automated subtitles for videos\n",
        "- Develop a voice-controlled application\n",
        "- Implement a podcast search engine\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **Whisper**: https://github.com/openai/whisper\n",
        "- **pyannote.audio**: https://github.com/pyannote/pyannote-audio\n",
        "- **librosa**: https://librosa.org/\n",
        "- **Hugging Face Models**: https://huggingface.co/models?pipeline_tag=automatic-speech-recognition\n",
        "\n",
        "### Feedback and Support\n",
        "\n",
        "Questions? Issues? Ideas?\n",
        "- Open an issue on GitHub\n",
        "- Check the documentation\n",
        "- Join the community discussions\n",
        "\n",
        "Thank you for participating in this workshop! \ud83c\udf89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Exercise: Build Your Own Application\n",
        "\n",
        "Use this space to build your own transcription application combining what you've learned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your final project code here\n",
        "# Build something amazing!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}